{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953166f-9996-4024-82e6-24ddf3effbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pv_vision_dir = os.path.join(Path.home(), 'pv-vision')\n",
    "pv_vision_dir = os.path.join('/home/eccoope', 'pv-vision')\n",
    "# functions_dir = os.path.join(Path.home(), 'el_img_cracks_ec', 'scripts')\n",
    "functions_dir = os.path.join('/home/eccoope', 'el_img_cracks_ec', 'scripts')\n",
    "\n",
    "sys.path.append(pv_vision_dir)\n",
    "sys.path.append(functions_dir)\n",
    "\n",
    "# ojas_functions_dir = os.path.join(Path.home(), 'pvcracks/retrain/')\n",
    "ojas_functions_dir = '/Users/ojas/Desktop/saj/SANDIA/pvcracks/retrain/'\n",
    "sys.path.append(ojas_functions_dir)\n",
    "\n",
    "from tutorials.unet_model import construct_unet\n",
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/Channeled_Combined_CWRU_LBNL_ASU_No_Empty/\"\n",
    "\n",
    "\n",
    "model_weight_paths = {\n",
    "    \"emma_retrained\": \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/retrained_pv-vision_model.pt\",\n",
    "    \"original\": \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/pv-vision_model.pt\",\n",
    "}\n",
    "\n",
    "# weight_path = model_weight_paths[\"emma_retrained\"]\n",
    "weight_path = model_weight_paths[\"original\"]\n",
    "\n",
    "checkpoint_name = root.split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b87524",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {0: \"dark\", 1: \"busbar\", 2: \"crack\", 3: \"non-cell\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b3df5-d827-4d5d-92a1-2c2fdcbef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root):\n",
    "    transformers = functions.Compose([functions.ChanneledFixResize(256), functions.ToTensor(), functions.Normalize()])\n",
    "    \n",
    "    train_dataset = functions.SolarDataset(root, image_folder=\"img/train\", \n",
    "            mask_folder=\"ann/train\", transforms=transformers)\n",
    "    \n",
    "    val_dataset = functions.SolarDataset(root, image_folder=\"img/val\", \n",
    "            mask_folder=\"ann/val\", transforms=transformers)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d70c97-24fa-427a-8934-932d695a6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_device_and_model(weight_path):\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "    unet = construct_unet(len(category_mapping))\n",
    "    unet = torch.nn.DataParallel(unet)\n",
    "    \n",
    "    model = unet.module.to(device)\n",
    "    \n",
    "    return device, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir(base_dir, checkpoint_name):\n",
    "    checkpoint_dir = base_dir + \"/checkpoints/\"\n",
    "    folders = [folder for folder in os.listdir(checkpoint_dir)]\n",
    "    \n",
    "    max_number = 0\n",
    "    for folder in folders:\n",
    "        number = int(folder[-1])\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "\n",
    "    new_folder_name = f\"{checkpoint_name}{max_number + 1}\"\n",
    "    new_folder_path = os.path.join(checkpoint_dir, new_folder_name)\n",
    "    \n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "    \n",
    "    return new_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = load_dataset(root)\n",
    "device, model = load_device_and_model(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d889e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def new_inference_and_show(idx, threshold=0.5):\n",
    "    # Get the preprocessed image and multi-hot ground truth mask\n",
    "    img, mask = train_loader.dataset.__getitem__(idx)\n",
    "    img = img.to(device)\n",
    "    \n",
    "    # Get the raw image for display (assuming __getraw__ returns a PIL image)\n",
    "    raw_img, _ = train_loader.dataset.__getraw__(idx)\n",
    "    \n",
    "    # --- Run inference ---\n",
    "    # Get raw logits from the model, then apply Sigmoid and threshold\n",
    "    logits = model(img.unsqueeze(0)).detach().cpu()  # shape: [1, 4, H, W]\n",
    "    probs = torch.sigmoid(logits)                     # shape: [1, 4, H, W]\n",
    "    pred_mask = (probs > threshold).float().squeeze(0).numpy()  # shape: [4, H, W]\n",
    "    \n",
    "    # Ground truth is assumed to be already a 4-channel multi-hot mask.\n",
    "    gt_mask = mask.cpu().numpy()  # shape: [4, H, W]\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    # Create a grid with 3 rows and 4 columns:\n",
    "    #   Row 0: Raw image (displayed only once in the first column)\n",
    "    #   Row 1: Ground truth masks for each class\n",
    "    #   Row 2: Predicted masks for each class\n",
    "    n_classes = len(category_mapping)\n",
    "    class_names = [f'({k}) {v}' for k, v in category_mapping.items()]\n",
    "    \n",
    "    fig, axs = plt.subplots(3, n_classes, figsize=(4*n_classes, 12))\n",
    "    \n",
    "    # Row 0: Display raw image in first subplot; hide other subplots in this row.\n",
    "    axs[0, 0].imshow(raw_img.convert('L'), cmap='viridis')\n",
    "    axs[0, 0].set_title(\"Raw Image\")\n",
    "    axs[0, 0].axis('off')\n",
    "    for j in range(1, n_classes):\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    # Row 1: Ground truth for each class (each channel)\n",
    "    for j in range(n_classes):\n",
    "        axs[1, j].imshow(gt_mask[j], cmap='viridis')\n",
    "        axs[1, j].set_title(f\"GT: {class_names[j]}\")\n",
    "        axs[1, j].axis('off')\n",
    "    \n",
    "    # Row 2: Predictions for each class (each channel)\n",
    "    for j in range(n_classes):\n",
    "        axs[2, j].imshow(pred_mask[j], cmap='viridis')\n",
    "        axs[2, j].set_title(f\"Pred: {class_names[j]}\")\n",
    "        axs[2, j].axis('off')\n",
    "    \n",
    "    fig.suptitle(\"Retrained Model Prediction\", fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f2288",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb9672-cea1-4261-86df-cc67308fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_val=4\n",
    "batch_size_train=4\n",
    "lr = 1e-4\n",
    "step_size=1\n",
    "gamma = 0.1\n",
    "num_epochs = 1\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "save_dir = get_save_dir(str(root), checkpoint_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "params_dict = {'batch_size_val' : batch_size_val,\n",
    "           'batch_size_train' : batch_size_train,\n",
    "           'lr' : lr,\n",
    "           'step_size' : step_size,\n",
    "           'gamma' : gamma,\n",
    "           'num_epochs' : num_epochs,\n",
    "           'criterion' : str(criterion)}\n",
    "\n",
    "with open(os.path.join(save_dir, 'params.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(params_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceacb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b8831-e729-42c3-9eae-703af3d3001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "# lr_scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "evaluate_metric=None\n",
    "running_record = {'train': {'loss': []}, 'val': {'loss': []}}\n",
    "\n",
    "save_name='model.pt'\n",
    "cache_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5a33b-f614-4c09-af94-1ab3de3f189d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_epoch_loss = []\n",
    "val_epoch_loss = []\n",
    "\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    \n",
    "    training_step_loss = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # calc loss -- bce with logits loss applies sigmoid interally\n",
    "        training_loss = criterion(output, target)\n",
    "        \n",
    "        #backward pass\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record loss\n",
    "        training_step_loss.append(training_loss.item())\n",
    "        \n",
    "    training_epoch_loss.append(np.array(training_step_loss).mean())\n",
    "\n",
    "    val_step_loss = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target.float()\n",
    "        \n",
    "        # forward pass\n",
    "        data = data.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # calc loss -- bce with logits loss applies sigmoid interally\n",
    "        val_loss = criterion(output, target)\n",
    "        \n",
    "        val_step_loss.append(val_loss.item())\n",
    "        \n",
    "    val_epoch_loss.append(np.array(val_step_loss).mean())\n",
    "    \n",
    "    os.makedirs(os.path.join(save_dir, f'epoch_{epoch}'), exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'epoch_{epoch}', save_name))\n",
    "    print(f'Saved model at epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32576c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     new_inference_and_show(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a9bfe-7b09-4a7a-8718-fc779cb8b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.arange(1, len(training_epoch_loss) + 1, 1)\n",
    "\n",
    "ax.scatter(x, training_epoch_loss, label='training loss')\n",
    "ax.scatter(x, val_epoch_loss, label='validation loss')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "\n",
    "print(training_epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_epoch_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
