{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953166f-9996-4024-82e6-24ddf3effbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from utils import train_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/Channeled_Combined_CWRU_LBNL_ASU/\"\n",
    "\n",
    "weight_path = \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/pv-vision_model.pt\"\n",
    "\n",
    "checkpoint_name = \"wandb_experiment_\" + root.split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326df646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, epsilon=1e-6):\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    dice = (2.0 * intersection + epsilon) / (union + epsilon)\n",
    "    return dice\n",
    "\n",
    "\n",
    "def iou_score(pred, target, epsilon=1e-6):\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + epsilon) / (union + epsilon)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_functions.load_dataset(root)\n",
    "device, model = train_functions.load_device_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {0: \"empty\", 1: \"dark\", 2: \"busbar\", 3: \"crack\", 4: \"non-cell\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd228e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wandb_image(idx, threshold=0.5):\n",
    "    # Get the preprocessed image and multi-hot ground truth mask\n",
    "    img, mask = train_loader.dataset.__getitem__(idx)\n",
    "    img = img.to(device)\n",
    "\n",
    "    # --- Run inference ---\n",
    "    # Get raw logits from the model, then apply Sigmoid and threshold\n",
    "    logits = model(img.unsqueeze(0)).detach().cpu()  # shape: [1, 5, H, W]\n",
    "    probs = torch.sigmoid(logits)  # shape: [1, 5, H, W]\n",
    "    pred_mask = (probs > threshold).float().squeeze(0).numpy()  # shape: [5, H, W]\n",
    "\n",
    "    # Ground truth is assumed to be already a 5-channel multi-hot mask.\n",
    "    gt_mask = mask.cpu().numpy()  # shape: [5, H, W]\n",
    "\n",
    "    n_classes = len(category_mapping)\n",
    "\n",
    "    this_id_mask_images = []\n",
    "    for i in range(n_classes):\n",
    "        masks_dict = {\n",
    "            \"predictions\": {\n",
    "                \"mask_data\": pred_mask[i],\n",
    "                \"class_labels\": category_mapping,\n",
    "            },\n",
    "            \"ground_truth\": {\n",
    "                \"mask_data\": gt_mask[i],\n",
    "                \"class_labels\": category_mapping,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        mask_img = wandb.Image(\n",
    "            img,\n",
    "            masks=masks_dict,\n",
    "        )\n",
    "        this_id_mask_images.append(mask_img)\n",
    "    return this_id_mask_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9681c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code that would save images from most recent epoch of model to Weights and Biases\n",
    "# Thought it would be useful to track visual progress of model over time; but just turned out to save too many images for us to meaningfully use. Instead just looked at the loss values to understand performance over time.\n",
    "\n",
    "\"\"\"\n",
    "def create_wandb_image_for_table(idx, threshold=0.5):\n",
    "    # Get the preprocessed image and multi-hot ground truth mask\n",
    "    img, mask = train_loader.dataset.__getitem__(idx)\n",
    "    img = img.to(device)\n",
    "\n",
    "    # --- Run inference ---\n",
    "    # Get raw logits from the model, then apply Sigmoid and threshold\n",
    "    logits = model(img.unsqueeze(0)).detach().cpu()  # shape: [1, 5, H, W]\n",
    "    probs = torch.sigmoid(logits)  # shape: [1, 5, H, W]\n",
    "    pred_mask = (probs > threshold).float().squeeze(0).numpy()  # shape: [5, H, W]\n",
    "\n",
    "    # Ground truth is assumed to be already a 5-channel multi-hot mask.\n",
    "    gt_mask = mask.cpu().numpy()  # shape: [5, H, W]\n",
    "\n",
    "    n_classes = len(category_mapping)\n",
    "\n",
    "    this_id_table_info = []\n",
    "    this_id_table_info.append(wandb.Image(img))\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        this_id_table_info.append(\n",
    "            wandb.Image(\n",
    "                img,\n",
    "                masks={\n",
    "                    \"ground_truth\": {\n",
    "                        \"mask_data\": gt_mask[i],\n",
    "                        \"class_labels\": {0: category_mapping[i]},\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "        this_id_table_info.append(\n",
    "            wandb.Image(\n",
    "                img,\n",
    "                masks={\n",
    "                    \"predictions\": {\n",
    "                        \"mask_data\": pred_mask[i],\n",
    "                        \"class_labels\": {0: category_mapping[i]},\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return this_id_table_info\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f2288",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb9672-cea1-4261-86df-cc67308fc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = train_functions.get_save_dir(str(root), checkpoint_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "original_config = {\n",
    "    \"batch_size_train\": 8,\n",
    "    \"lr\": 0.00092234,\n",
    "    \"gamma\": 0.11727,\n",
    "    \"num_epochs\": 45,\n",
    "    \"batch_size_val\": 8,\n",
    "    \"criterion\": torch.nn.BCEWithLogitsLoss(),\n",
    "}\n",
    "\n",
    "config_serializable = original_config.copy()\n",
    "config_serializable[\"criterion\"] = str(config_serializable[\"criterion\"])\n",
    "\n",
    "with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_serializable, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"pvcracks\",\n",
    "    entity=\"ojas-sanghi-university-of-arizona\",\n",
    "    config=original_config,\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceacb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size_train, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b8831-e729-42c3-9eae-703af3d3001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "save_name = \"model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0725d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log gradients\n",
    "run.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5a33b-f614-4c09-af94-1ab3de3f189d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_epoch_loss = []\n",
    "val_epoch_loss = []\n",
    "\n",
    "for epoch in tqdm(range(1, config.num_epochs + 1)):\n",
    "    training_step_loss = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # calc loss -- bce with logits loss applies sigmoid interally\n",
    "        training_loss = original_config[\"criterion\"](output, target)\n",
    "\n",
    "        # backward pass\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record loss\n",
    "        training_step_loss.append(training_loss.item())\n",
    "\n",
    "    training_epoch_loss.append(np.array(training_step_loss).mean())\n",
    "\n",
    "    val_step_loss = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target.float()\n",
    "\n",
    "        # forward pass\n",
    "        data = data.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # calc loss -- bce with logits loss applies sigmoid interally\n",
    "        val_loss = original_config[\"criterion\"](output, target)\n",
    "\n",
    "        val_step_loss.append(val_loss.item())\n",
    "\n",
    "    val_epoch_loss.append(np.array(val_step_loss).mean())\n",
    "\n",
    "    # Compute dice and IoU metrics per channel\n",
    "    pred_probs = torch.sigmoid(output)\n",
    "    pred_binary = (pred_probs > 0.5).float()\n",
    "\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "\n",
    "    for i in range(pred_binary.size(1)):  # Loop over channels\n",
    "        dice = dice_coefficient(pred_binary[:, i], target[:, i])\n",
    "        iou = iou_score(pred_binary[:, i], target[:, i])\n",
    "        dice_scores.append(dice.item())\n",
    "        iou_scores.append(iou.item())\n",
    "\n",
    "    avg_dice = np.mean(dice_scores)\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{config.num_epochs}, Training Loss: {np.array(training_step_loss).mean()}, Validation Loss: {np.array(val_step_loss).mean()}, Avg Dice: {avg_dice}, Avg IoU: {avg_iou}\"\n",
    "    )\n",
    "\n",
    "    # see above for explanation of why this is commented out\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions for wandb...\")\n",
    "    mask_images = []\n",
    "    table = wandb.Table(\n",
    "        columns=[\n",
    "            \"Image\",\n",
    "            \"GT Empty\",\n",
    "            \"Pred Empty\",\n",
    "            \"GT Dark\",\n",
    "            \"Pred Dark\",\n",
    "            \"GT Busbar\",\n",
    "            \"Pred Busbar\",\n",
    "            \"GT Crack\",\n",
    "            \"Pred Crack\",\n",
    "            \"GT Non-cell\",\n",
    "            \"Pred Non-cell\",\n",
    "        ]\n",
    "    )\n",
    "    for id in range(20):\n",
    "        mask_images.extend(create_wandb_image(id))\n",
    "        new_img_table = create_wandb_image_for_table(id)\n",
    "        table.add_data(*new_img_table)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Logging to wandb...\")\n",
    "    run.log(\n",
    "        {\n",
    "            \"train_loss\": np.array(training_step_loss).mean(),\n",
    "            \"val_loss\": np.array(val_step_loss).mean(),\n",
    "            # \"predictions\": mask_images,\n",
    "            # \"table\": table,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    table = wandb.Table(columns=[\"Image\"])\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    os.makedirs(os.path.join(save_dir, f\"epoch_{epoch}\"), exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"epoch_{epoch}\", save_name))\n",
    "    print(f\"Saved model at epoch {epoch}.\", end=\" \")\n",
    "\n",
    "    if epoch >= 2 and epoch < config.num_epochs:\n",
    "        os.remove(os.path.join(save_dir, f\"epoch_{epoch - 1}\", save_name))\n",
    "        print(f\"Removed model at epoch {epoch - 1}.\", end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06593fb9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c27bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvcracks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
