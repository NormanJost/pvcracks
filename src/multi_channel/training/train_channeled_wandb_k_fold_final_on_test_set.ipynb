{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2953166f-9996-4024-82e6-24ddf3effbfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m ojas_functions_dir = \u001b[33m'\u001b[39m\u001b[33m/Users/ojas/Desktop/saj/SANDIA/pvcracks/retrain/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     30\u001b[39m sys.path.append(ojas_functions_dir)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtutorials\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munet_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m construct_unet\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctions\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m random_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tutorials'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pv_vision_dir = os.path.join(Path.home(), 'pv-vision')\n",
    "pv_vision_dir = os.path.join(\"/home/eccoope\", \"pv-vision\")\n",
    "# functions_dir = os.path.join(Path.home(), 'el_img_cracks_ec', 'scripts')\n",
    "functions_dir = os.path.join(\"/home/eccoope\", \"el_img_cracks_ec\", \"scripts\")\n",
    "\n",
    "sys.path.append(pv_vision_dir)\n",
    "sys.path.append(functions_dir)\n",
    "\n",
    "# ojas_functions_dir = os.path.join(Path.home(), 'pvcracks/retrain/')\n",
    "ojas_functions_dir = \"/Users/ojas/Desktop/saj/SANDIA/pvcracks/retrain/\"\n",
    "sys.path.append(ojas_functions_dir)\n",
    "\n",
    "from utils.unet_model import construct_unet\n",
    "import functions\n",
    "from torch.utils.data import random_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/Channeled_Combined_CWRU_LBNL_ASU_No_Empty/\"\n",
    "\n",
    "\n",
    "model_weight_paths = {\n",
    "    \"emma_retrained\": \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/retrained_pv-vision_model.pt\",\n",
    "    \"original\": \"/Users/ojas/Desktop/saj/SANDIA/pvcracks_data/pv-vision_model.pt\",\n",
    "}\n",
    "\n",
    "# weight_path = model_weight_paths[\"emma_retrained\"]\n",
    "weight_path = model_weight_paths[\"original\"]\n",
    "\n",
    "checkpoint_name = root.split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b87524",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {0: \"dark\", 1: \"busbar\", 2: \"crack\", 3: \"non-cell\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf350952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, epsilon=1e-6):\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    dice = (2.0 * intersection + epsilon) / (union + epsilon)\n",
    "    return dice\n",
    "\n",
    "\n",
    "def iou_score(pred, target, epsilon=1e-6):\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + epsilon) / (union + epsilon)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b3df5-d827-4d5d-92a1-2c2fdcbef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root):\n",
    "    transformers = functions.Compose(\n",
    "        [functions.ChanneledFixResize(256), functions.ToTensor(), functions.Normalize()]\n",
    "    )\n",
    "\n",
    "    full_dataset = functions.SolarDataset(\n",
    "        root, image_folder=\"img/all\", mask_folder=\"ann/all\", transforms=transformers\n",
    "    )\n",
    "\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d70c97-24fa-427a-8934-932d695a6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_device_and_model(weight_path):\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "    unet = construct_unet(len(category_mapping))\n",
    "    unet = torch.nn.DataParallel(unet)\n",
    "\n",
    "    model = unet.module.to(device)\n",
    "\n",
    "    return device, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir(base_dir, checkpoint_name):\n",
    "    checkpoint_dir = base_dir + \"/checkpoints/\"\n",
    "    folders = [folder for folder in os.listdir(checkpoint_dir)]\n",
    "\n",
    "    max_number = 0\n",
    "    for folder in folders:\n",
    "        number = int(folder[-1])\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "\n",
    "    new_folder_name = f\"{checkpoint_name}{max_number + 1}\"\n",
    "    new_folder_path = os.path.join(checkpoint_dir, new_folder_name)\n",
    "\n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "    return new_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = load_dataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_subset, test_subset = random_split(\n",
    "    full_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is needed so that we can a) split the dataset into train/test while ensuring our seed is the same as the wandb_k_fold, b) and preserver stuff like __getraw__ from solardataset when doing inference\n",
    "\n",
    "\n",
    "class SubsetWithRaw(torch.utils.data.Subset):\n",
    "    def __getraw__(self, idx):\n",
    "        return self.dataset.__getraw__(self.indices[idx])\n",
    "\n",
    "\n",
    "train_set = SubsetWithRaw(full_dataset, train_subset.indices)\n",
    "test_set = SubsetWithRaw(full_dataset, test_subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, model = load_device_and_model(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d889e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def new_inference_and_show(idx, threshold=0.5):\n",
    "    # Get the preprocessed image and multi-hot ground truth mask\n",
    "    img, mask = test_loader.dataset.__getitem__(idx)\n",
    "    img = img.to(device)\n",
    "\n",
    "    # Get the raw image for display (assuming __getraw__ returns a PIL image)\n",
    "    raw_img, _ = test_loader.dataset.__getraw__(idx)\n",
    "\n",
    "    # --- Run inference ---\n",
    "    # Get raw logits from the model, then apply Sigmoid and threshold\n",
    "    logits = model(img.unsqueeze(0)).detach().cpu()  # shape: [1, 4, H, W]\n",
    "    probs = torch.sigmoid(logits)  # shape: [1, 4, H, W]\n",
    "    pred_mask = (probs > threshold).float().squeeze(0).numpy()  # shape: [4, H, W]\n",
    "\n",
    "    # Ground truth is assumed to be already a 4-channel multi-hot mask.\n",
    "    gt_mask = mask.cpu().numpy()  # shape: [4, H, W]\n",
    "\n",
    "    # --- Visualization ---\n",
    "    # Create a grid with 3 rows and 4 columns:\n",
    "    #   Row 0: Raw image (displayed only once in the first column)\n",
    "    #   Row 1: Ground truth masks for each class\n",
    "    #   Row 2: Predicted masks for each class\n",
    "    n_classes = len(category_mapping)\n",
    "    class_names = [f\"({k}) {v}\" for k, v in category_mapping.items()]\n",
    "\n",
    "    fig, axs = plt.subplots(3, n_classes, figsize=(4 * n_classes, 12))\n",
    "\n",
    "    # Row 0: Display raw image in first subplot; hide other subplots in this row.\n",
    "    axs[0, 0].imshow(raw_img.convert(\"L\"), cmap=\"viridis\")\n",
    "    axs[0, 0].set_title(\"Raw Image\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "    for j in range(1, n_classes):\n",
    "        axs[0, j].axis(\"off\")\n",
    "\n",
    "    # Row 1: Ground truth for each class (each channel)\n",
    "    for j in range(n_classes):\n",
    "        axs[1, j].imshow(gt_mask[j], cmap=\"viridis\")\n",
    "        axs[1, j].set_title(f\"GT: {class_names[j]}\")\n",
    "        axs[1, j].axis(\"off\")\n",
    "\n",
    "    # Row 2: Predictions for each class (each channel)\n",
    "    for j in range(n_classes):\n",
    "        axs[2, j].imshow(pred_mask[j], cmap=\"viridis\")\n",
    "        axs[2, j].set_title(f\"Pred: {class_names[j]}\")\n",
    "        axs[2, j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Retrained Model Prediction\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f2288",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03596da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"model.pt\"\n",
    "save_dir = get_save_dir(str(root), checkpoint_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "original_config = {\n",
    "    \"batch_size_train\": 8,\n",
    "    \"lr\": 0.00092234,\n",
    "    \"gamma\": 0.11727,\n",
    "    \"num_epochs\": 2,\n",
    "    # constants\n",
    "    \"batch_size_test\": 8,\n",
    "    \"criterion\": torch.nn.BCEWithLogitsLoss(),\n",
    "    \"k_folds\": 5,\n",
    "    # \"lr_scheduler_step_size\": 1,\n",
    "}\n",
    "\n",
    "config_serializable = original_config.copy()\n",
    "config_serializable[\"criterion\"] = str(config_serializable[\"criterion\"])\n",
    "\n",
    "with open(os.path.join(save_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_serializable, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"pvcracks\",\n",
    "    entity=\"ojas-sanghi-university-of-arizona\",\n",
    "    config=original_config,\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=config.batch_size_train, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=config.batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b8831-e729-42c3-9eae-703af3d3001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# log gradients\n",
    "run.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5a33b-f614-4c09-af94-1ab3de3f189d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_dice_loss = []\n",
    "test_iou_loss = []\n",
    "\n",
    "best_epoch_test_loss = float(\"inf\")\n",
    "best_epoch_dice = 0.0\n",
    "best_epoch_iou = 0.0\n",
    "\n",
    "for epoch in tqdm(range(1, config.num_epochs + 1)):\n",
    "    training_step_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # calc loss -- bce with logits loss applies sigmoid interally\n",
    "        training_loss = original_config[\"criterion\"](output, target)\n",
    "        # backward pass\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "        # record loss\n",
    "        training_step_loss.append(training_loss.item())\n",
    "\n",
    "    test_step_loss = []\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target = target.float()\n",
    "        # forward pass\n",
    "        # data = data.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        # calc loss -- bce with logits loss applies sigmoid interally\n",
    "        test_loss = original_config[\"criterion\"](output, target)\n",
    "        test_step_loss.append(test_loss.item())\n",
    "\n",
    "        # compute dice and iou\n",
    "        pred_probs = torch.sigmoid(output)\n",
    "        pred_binary = (pred_probs > 0.5).float()\n",
    "        for i in range(pred_binary.size(1)):\n",
    "            dice = dice_coefficient(pred_binary[:, i], target[:, i])\n",
    "            iou = iou_score(pred_binary[:, i], target[:, i])\n",
    "            dice_scores.append(dice.item())\n",
    "            iou_scores.append(iou.item())\n",
    "\n",
    "    epoch_train_loss = np.mean(training_step_loss)\n",
    "    epoch_test_loss = np.mean(test_step_loss)\n",
    "    epoch_avg_dice = np.mean(dice_scores)\n",
    "    epoch_avg_iou = np.mean(iou_scores)\n",
    "\n",
    "    training_epoch_loss.append(epoch_train_loss)\n",
    "    test_epoch_loss.append(epoch_test_loss)\n",
    "    test_dice_loss.append(epoch_avg_dice)\n",
    "    test_iou_loss.append(epoch_avg_iou)\n",
    "\n",
    "    run.log(\n",
    "        {\n",
    "            \"train_loss\": epoch_train_loss,\n",
    "            \"test_loss\": epoch_test_loss,\n",
    "            \"avg_dice\": epoch_avg_dice,\n",
    "            \"avg_iou\": epoch_avg_iou,\n",
    "        },\n",
    "        step=epoch,\n",
    "    )\n",
    "\n",
    "    if epoch_test_loss < best_epoch_test_loss:\n",
    "        best_epoch_test_loss = epoch_test_loss\n",
    "        best_epoch_dice = epoch_avg_dice\n",
    "        best_epoch_iou = epoch_avg_iou\n",
    "\n",
    "        os.makedirs(os.path.join(save_dir, f\"epoch_{epoch}\"), exist_ok=True)\n",
    "        torch.save(\n",
    "            model.state_dict(), os.path.join(save_dir, f\"epoch_{epoch}\", save_name)\n",
    "        )\n",
    "        print(f\"Saved model at epoch {epoch}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} best test_loss: {best_epoch_test_loss:.4f}, dice: {best_epoch_dice:.4f}, iou: {best_epoch_iou:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e08d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32576c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inference_and_show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     new_inference_and_show(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a9bfe-7b09-4a7a-8718-fc779cb8b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.arange(1, len(training_epoch_loss) + 1, 1)\n",
    "\n",
    "ax.scatter(x, training_epoch_loss, label=\"training loss\")\n",
    "ax.scatter(x, test_epoch_loss, label=\"test loss\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "print(training_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvcracks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
